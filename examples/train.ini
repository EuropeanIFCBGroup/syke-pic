[dataset]
path = /home/ubuntu/labeled_2020
split = 0.6, 0.2, 0.2
; Minimun class size
min_N =
; Maximum class size, useful for test runs
max_N =
exclude =
; Random seed enables the use of same train-val-test split
random_seed = 24
oversample = yes
; Oversamples training samples until this is reached
oversample_until = 100
; Oversamples training samples with this exponential decay
oversample_decay = 0.995

[model]
; Directory in which to save trained models
path = /home/ubuntu/models/resnet18
; Only ResNets should be used currently
network = resnet18
; Use automatic version numbering, i.e version_1, version_2 etc.
id = auto
; Allow overwriting saved models, won't happen when id = auto
exist_ok = no
; Head (last) layers inserted on top of a pre-trained network
head = 256, 128
; idx, prob; idx, prob etc.
; e.g. -2, 0.5 would insert Dropout(0.5) before second to last fc layer
dropout =

[image]
; labeled_2020 mean width: ~180, height: ~70
shape = 3, 180, 180
; Options: flip, translate, zoom, rotate, brightness
; NOTE! Order matters (brightness should always be last)
augmentations = flip, translate, zoom, brightness
imagenet_normalization = no
; Options: mode, black, white
border = mode
zoom_range = 0.6, 1.4
brightness_range = 0.95, 1.1
max_rotation = 10
batch_size = 64
; Parameter for PyTorch's DataLoader
num_workers = 2

[train]
gpu = yes
max_epochs = 100
early_stop_patience = 12
learning_rate = 0.01
optimizer = Adam
; CrossEntropy is the default loss function

[lr_warmup]
; LRWarmup located in ifcb/module_tools.py
use = yes
factor_1 = 0.1
factor_2 = 0.5
step_1 = 4
step_2 = 14
step_3 = 24
verbose = yes

[lr_reduction]
; ReduceLROnPlateau scheduler (monitors validation loss)
use = yes
factor = 0.1
; Actual reduction at epoch: patience+1
; When using lr_warmup, patience starts counting only after lr_warmup's final step
patience = 4
verbose = yes